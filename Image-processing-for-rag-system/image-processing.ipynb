{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13afc29d-88cb-4926-bee6-652df387c3e9",
   "metadata": {},
   "source": [
    "# Image processing in RAG system\n",
    "\n",
    "Traditionally, RAG systems focus on textual inputs, but the inclusion of image processing can extend their functionality to multimodal scenarios. This notebook examines three methods for integrating image processing into a RAG system:\n",
    "\n",
    "- Optical Character Recognition for text extraction\n",
    "- Vision models for image description\n",
    "- Multimodal embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55b6d7-56d5-44ae-8275-b0ad3fa01506",
   "metadata": {},
   "source": [
    "# Setup 0: Text Embedding\n",
    "\n",
    "for embeddings the output text of each method we use nomic embed text v1.5 using lm-studio.\n",
    "\n",
    "we use default config of lm-studio settings that provided in **Local Server**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf93a10-8079-48db-afdd-b0ad600e735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np \n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_embedding(text, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    embedding = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "    return np.array([embedding])\n",
    "\n",
    "test_embedding = get_embedding(\"Boo ...\")\n",
    "test_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0584c65-7678-4425-a2af-863a05d37114",
   "metadata": {},
   "source": [
    "Now lets save image paths with their query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1abc44-cc0f-4d80-b6b8-f98e39b9ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/documentation.png\", \"data/drag-race.jpeg\", \"data/orange-juice.jpeg\"]\n",
    "queries = [\"give me an overview about software design\", \"orange juice with pulp produced in USA\", \"drag race between to classic car\"]\n",
    "\n",
    "embedded_queries = []\n",
    "for query in queries:\n",
    "    embedded_queries.append(get_embedding(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d166766-d96f-4db6-9b62-07b165431920",
   "metadata": {},
   "source": [
    "# OCR-Based Text Extraction\n",
    "\n",
    "This method uses the OCR technique, as it first extracts text from the image and then converts them into vectors using embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d771830-a4c8-4df1-bcde-0cae8e23801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khabith/Files/Github/Rag-System/rag/lib/python3.10/site-packages/doctr/models/utils/pytorch.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(archive_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor\n",
    "\n",
    "model = ocr_predictor(pretrained=True)\n",
    "def extract_texts(image_path):\n",
    "    img_doc = DocumentFile.from_images(image_path)\n",
    "    return model(img_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb89d1b-e11e-4f02-9681-6fce7e07aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_texts = []\n",
    "\n",
    "for path in paths: \n",
    "    text = extract_texts(path)\n",
    "    ocr_texts.append(text.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bee68185-f4ee-474d-bede-c24aa47f1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_texts_embeddings = []\n",
    "\n",
    "for text in ocr_texts:\n",
    "    ocr_texts_embeddings.append(get_embedding(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d4610c0-8dde-4d1f-b163-22989fb4a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "ocr_similarity = pd.DataFrame(index=range(len(ocr_texts_embeddings)), columns=range(len(embedded_queries)), dtype=float)\n",
    "\n",
    "for num_col in range(len(embedded_queries)):\n",
    "    for num_row in range(num_col, len(embedded_queries)):\n",
    "        a = embedded_queries[num_col]\n",
    "        b = ocr_texts_embeddings[num_row]\n",
    "        similarity = cosine_similarity(a,b)[0][0]\n",
    "\n",
    "        ocr_similarity.loc[num_col, num_row] = similarity\n",
    "        ocr_similarity.loc[num_row, num_col] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "327000ba-a104-47fb-97a8-cda32d0b73e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>documentation.png</th>\n",
       "      <td>0.709916</td>\n",
       "      <td>0.316771</td>\n",
       "      <td>0.390691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drag-race.jpeg</th>\n",
       "      <td>0.316771</td>\n",
       "      <td>0.351983</td>\n",
       "      <td>0.719673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orange-juice.jpeg</th>\n",
       "      <td>0.390691</td>\n",
       "      <td>0.719673</td>\n",
       "      <td>0.437930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1         2\n",
       "documentation.png  0.709916  0.316771  0.390691\n",
       "drag-race.jpeg     0.316771  0.351983  0.719673\n",
       "orange-juice.jpeg  0.390691  0.719673  0.437930"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_df = ocr_similarity.rename({i: paths[i].replace('data/', '') for i in ocr_similarity.index})\n",
    "ocr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ab7c8-af1f-4fd1-976b-e307429387d7",
   "metadata": {},
   "source": [
    "# Vision-Based Models for Image Captioning\n",
    "\n",
    "In this method, vision models such as convolutional neural networks or versatile models such as GPT-4O are used to describe an image. These descriptions are then converted into vectors using embedding models.\n",
    "\n",
    "for now we use [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf) and lm-studio to connect to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d61b962e-be1c-4c82-a3a5-de2144515300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "\n",
    "def describe_image(path): \n",
    "    image = open(path.replace(\"'\", \"\"), \"rb\").read()\n",
    "    base64_image = base64.b64encode(image).decode(\"utf-8\")\n",
    "\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"model-identifier\",\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"You are an intelligent assistant. You are helping the user to describe an image. Provide only the answer; avoid unnecessary talk or explanations.\",\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"describe this image and if there is any content give me a summary of it.\"},\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "              },\n",
    "            },\n",
    "          ],\n",
    "        }\n",
    "      ],\n",
    "      max_tokens=1000,\n",
    "      stream=True\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            full_response += chunk.choices[0].delta.content\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a767f59-308a-4ade-b8bd-39f34c9611df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/documentation.png\n",
      "The image you've shared is a screenshot of a software design document. The document is neatly organized into three sections, each with a distinct purpose.\n",
      "\n",
      "Starting from the top left corner, we see a section titled \"Software Design Document\". This section has a blue header and white text, providing clear contrast for easy reading. It also contains instructions on how to describe an image in this context.\n",
      "\n",
      "Moving to the right side of the document, we find another section titled \"System Overview\". This section follows a similar layout with a blue header and white text. It provides a brief overview of the system design, setting the stage for the rest of the document.\n",
      "\n",
      "Finally, at the bottom center of the document, we see the third section titled \"Design Considerations\". This section also has a blue header and white text. It outlines several key considerations in designing software.\n",
      "\n",
      "The document is written in English and includes some technical terms related to software design. The overall layout suggests a professional and structured approach to software design planning.data/drag-race.jpeg\n",
      "The image captures a dynamic scene on a racetrack. Two classic cars, one with a silver body adorned with black stripes and the other boasting a brown color with white stripes, are caught in motion against the backdrop of a red track bordered by a white curb.\n",
      "\n",
      "The car on the left, painted in shades of silver and black, is positioned slightly ahead of its competitor, suggesting it's leading the race. Its counterpart, the brown car with white stripes, trails behind, perhaps indicating a slight lag in the race.\n",
      "\n",
      "The racetrack itself is lined with red barriers, providing a stark contrast to the cars' colors and enhancing the sense of speed and competition. Beyond the track, a grassy field stretches out, leading up to stands filled with spectators, their presence adding to the atmosphere of an ongoing race. The image as a whole encapsulates the thrill and excitement of a car race.data/orange-juice.jpeg\n",
      "The image showcases a vibrant orange juice carton from Florida's Natural, a cooperative owned by the state's citrus growers. The carton, standing upright against a stark white background, is predominantly orange in color, reflecting the contents within.\n",
      "\n",
      "At the top of the carton, there's a label featuring two ripe oranges and a green leaf, symbolizing the source of the juice. The brand name \"Florida's Natural\" is prominently displayed in bold black text at the top of the carton, asserting its identity.\n",
      "\n",
      "Just below the brand name, there's a promise of quality written in white text: \"100% PURE\", followed by \"FRESH SQUEESED ORANGE JUICE\". This suggests that the juice inside is made from 100% fresh squeezed oranges, maintaining its purity and freshness.\n",
      "\n",
      "On the right side of the carton, there's another label with the words \"Premium Orange Juice\" written in black text. This label might indicate a higher quality or more expensive variant of the orange juice offered by this cooperative.\n",
      "\n",
      "The overall design of the carton is simple yet effective, with clear text and images conveying information about the product inside. The use of bright colors like orange and green not only adds to its visual appeal but also reinforces the idea that it contains freshly squeezed orange juice."
     ]
    }
   ],
   "source": [
    "vision_descriptions = []\n",
    "\n",
    "for path in paths: \n",
    "    print(path)\n",
    "    text = describe_image(path)\n",
    "    vision_descriptions.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc07321b-894b-4919-8c04-438422b65f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_descriptions_embeddings = []\n",
    "\n",
    "for description in vision_descriptions:\n",
    "    vision_descriptions_embeddings.append(get_embedding(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d3b49db0-494d-424d-b8e4-0c41acfc5acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>documentation.png</th>\n",
       "      <td>0.741236</td>\n",
       "      <td>0.372204</td>\n",
       "      <td>0.418964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drag-race.jpeg</th>\n",
       "      <td>0.372204</td>\n",
       "      <td>0.392488</td>\n",
       "      <td>0.668240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orange-juice.jpeg</th>\n",
       "      <td>0.418964</td>\n",
       "      <td>0.668240</td>\n",
       "      <td>0.419419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0         1         2\n",
       "documentation.png  0.741236  0.372204  0.418964\n",
       "drag-race.jpeg     0.372204  0.392488  0.668240\n",
       "orange-juice.jpeg  0.418964  0.668240  0.419419"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_similarity = pd.DataFrame(index=range(len(vision_descriptions_embeddings)), columns=range(len(embedded_queries)), dtype=float)\n",
    "\n",
    "for num_col in range(len(embedded_queries)):\n",
    "    for num_row in range(num_col, len(embedded_queries)):\n",
    "        a = embedded_queries[num_col]\n",
    "        b = vision_descriptions_embeddings[num_row]\n",
    "        similarity = cosine_similarity(a,b)[0][0]\n",
    "\n",
    "        vision_similarity.loc[num_col, num_row] = similarity\n",
    "        vision_similarity.loc[num_row, num_col] = similarity\n",
    "\n",
    "vision_df = vision_similarity.rename({i: paths[i].replace('data/', '') for i in vision_similarity.index})\n",
    "vision_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb579d8b-58bb-4368-8ab5-e2b37600ec07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ca3fdf-858d-4cb7-9204-6b9795eda6e7",
   "metadata": {},
   "source": [
    "# Direct Image Embedding\n",
    "In this method, images are directly converted into vectors by embedding models and can be used separately in the RAG system.\n",
    "\n",
    "we use [CLIP](https://github.com/openai/CLIP) for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84eeea9e-9c0c-4d15-aeee-b56bab56aa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khabith/Files/Github/Rag-System/rag/lib/python3.10/site-packages/clip/clip.py:57: UserWarning: /Users/khabith/.cache/clip/ViT-B-32.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "100%|███████████████████████████████████████| 338M/338M [04:43<00:00, 1.25MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e154d4c-b792-4995-a30e-3d989458e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clip.tokenize(queries).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "text_vector = text_features.cpu().numpy()\n",
    "\n",
    "\n",
    "def process(path):\n",
    "    image = preprocess(Image.open(path)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    image_vector = image_features.cpu().numpy()\n",
    "\n",
    "    return (image_features @ text_features.T).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5baa35-bc1a-4d95-ba0c-a87395fc9b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/documentation.png', 'data/drag-race.jpeg', 'data/orange-juice.jpeg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0ea022-3bf8-4623-95ea-6796daa5273c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[1;32m      6\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m process(path)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43membedding_similarity\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m similarity\n\u001b[1;32m      9\u001b[0m embedding_similarity\n",
      "File \u001b[0;32m~/Files/Github/Rag-System/rag/lib/python3.10/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Files/Github/Rag-System/rag/lib/python3.10/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/Files/Github/Rag-System/rag/lib/python3.10/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/Files/Github/Rag-System/rag/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (1) does not match length of index (3)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embedding_similarity = pd.DataFrame(index=range(len(text_vector)), columns=range(len(text_vector)), dtype=float)\n",
    "\n",
    "for path in paths:\n",
    "    similarity = process(path)\n",
    "    embedding_similarity[path.replace('data/', '')] = similarity\n",
    "\n",
    "embedding_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0188039e-f06d-40ab-9fb6-283ca9f2ed67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vector.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
